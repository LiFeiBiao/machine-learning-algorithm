## Adaboost：基于错误提升分类器的性能
AbaBoost是adaptive boosting的缩写，其运行过程如下：训练数据中的每个样本，并赋予其一个权重，这些权重构成一个向量D。
一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，
然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重
将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AbaBoost为每个分类器都分配了一
个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。



介绍:此外作者还有一篇[元算法、AdaBoost　python实现文章](http://blog.csdn.net/suipingsp/article/details/41722435)